{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chrome\n",
    "chrome_options = Options()\n",
    "\n",
    "#opens the website\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install()) #gets the new version of chrome \n",
    "driver.maximize_window()\n",
    "\n",
    "url = \"https://forums.edmunds.com/discussion/2444/general/x/what-car-is-right-for-me-help-me-choose\"\n",
    "driver.get(url)\n",
    "\n",
    "\n",
    "#function to click next \n",
    "def click_next():\n",
    "    try: \n",
    "        element = driver.find_element_by_css_selector(\"[title*='Next Page']\")\n",
    "        element.click()\n",
    "    except IndexError: \n",
    "        print(\"Next button not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_list = []\n",
    "users_list = []\n",
    "dates_list = []\n",
    "nbposts_list = []\n",
    "roles_list = []\n",
    "\n",
    "num_pages = 84\n",
    "\n",
    "for page_count in range(1,num_pages):\n",
    "    time.sleep(1)    \n",
    "    page = driver.page_source\n",
    "    soup = BeautifulSoup(page,'lxml')\n",
    "\n",
    "    messages = soup.find_all('div', class_='Message userContent')\n",
    "    user_ids = soup.find_all('a', class_='Username js-userCard')\n",
    "    dates = soup.find_all('span', class_='MItem DateCreated')\n",
    "    roles = soup.find_all('span', class_='MItem RoleTitle')\n",
    "    nbposts = soup.find_all('span', class_='MItem PostCount')\n",
    "    \n",
    "\n",
    "    #appending messages \n",
    "    for i in messages:\n",
    "            messages_list.append(i.get_text())\n",
    "\n",
    "    #appending dates\n",
    "    for i in dates:\n",
    "            dates_list.append(i.get_text())\n",
    "            \n",
    "    #appending users\n",
    "    for i in user_ids: \n",
    "        users_list.append(i.get_text())\n",
    "        \n",
    "    #appending user nb of posts\n",
    "    for i in nbposts: \n",
    "        nbposts_list.append(i.get_text())\n",
    "    \n",
    "    #appending member role\n",
    "    for i in roles: \n",
    "        roles_list.append(i.get_text())\n",
    "    \n",
    "\n",
    "    print(\"Done scraping\", page_count, \"of 84 pages.\")\n",
    "    click_next()\n",
    "    time.sleep(1)\n",
    "    \n",
    "df = pd.DataFrame({\"Date\": dates_list, \"User_Id\": users_list, \"Message\": messages_list, \"NumberOfPastPosts\" : nbposts_list, \"Role\" : roles_list})\n",
    "df.to_csv('data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "df = df.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning\n",
    "#dates\n",
    "from datetime import datetime\n",
    "df['Date'] = df['Date'].map(lambda x: x.lstrip('\\n'))\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "#messages\n",
    "df['Message'] = df['Message'].map(lambda x: x.lstrip('\\n'))\n",
    "#nb posts\n",
    "df['NumberOfPastPosts'] = df['NumberOfPastPosts'].map(lambda x: x.lstrip('Posts: '))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the text processing method to eliminate punctuation and stopwords\n",
    "\n",
    "import string\n",
    "\n",
    "#importing stopwords from the natural language toolkit library \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def text_process(mess):\n",
    "    nopunc =[char for char in mess if char not in string.punctuation]\n",
    "    nopunc=''.join(nopunc)\n",
    "    final_list = []\n",
    "    for word in nopunc.split():\n",
    "        if word.lower() not in stopwords.words('english'):\n",
    "            final_list.append(word)\n",
    "    return final_list\n",
    "\n",
    "df['Message_words'] = df['Message'].apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib \n",
    "\n",
    "spam_model = joblib.load('spam_detector_model.pkl')\n",
    "spam_xtrain = joblib.load('xtrain.pkl')\n",
    "spam_ytrain = joblib.load('ytrain.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing our model and methods\n",
    "\n",
    "#we choose to work with a multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#CountVectorizer converts a collection of text documents to a matrix of token counts\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#The TfidfTransformer converts a collection of documents to a matrix of TF-IDF features. \n",
    "# TFIDF is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#pipeline creation to accelerate sample predictions\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "spam_pipeline = Pipeline([\n",
    "   ( 'bow',CountVectorizer(analyzer=text_process)),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('classifier',spam_model),\n",
    "])\n",
    "\n",
    "\n",
    "spam_pipeline.fit(spam_xtrain,spam_ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModerateMyPost(post):\n",
    "    sample_spam_predict = spam_pipeline.predict([post])\n",
    "    my_mod = {'label' : str(sample_spam_predict) }\n",
    "    return my_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "\n",
    "df_notnan = df[df['Message'].notna()]\n",
    "\n",
    "\n",
    "labels = []\n",
    "i=0\n",
    "for post in df_notnan.Message:\n",
    "    sample_spam_predict = spam_pipeline.predict([post])\n",
    "    labels.append(sample_spam_predict)\n",
    "    print('Checked message', i, ' of', len(df_notnan.Message))\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "df_notnan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notnan['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for potential_spam in df_notnan[df_notnan['label'] == \"spam\"]['Message']:\n",
    "    print(\"\\n\", potential_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_notnan\n",
    "\n",
    "\n",
    "joblib.dump(df_clean, \"df_clean.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
