{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 1000) # to read records completely\n",
    "import pickle\n",
    "import numpy as np\n",
    "import itertools\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data from previous preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing scraped and pre-processed data  \n",
    "df = pd.read_pickle('data_clean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.read_csv('models.csv', header=None, names = ['brand', 'model'])\n",
    "models['brand'] = models['brand'].str.replace(r'[^\\w\\s]+', '', regex=True) # remove punctuation errors (e.g. \"nissan.\")\n",
    "models = models[~models.brand.isin(['car', 'sedan', 'problem'])] # remove rows that are not brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all message words to lowercase\n",
    "df.Message_words = df.Message_words.apply(lambda x: [i.lower() for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace models with brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = df.Message_words.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace first occurence of brand\n",
    "messages2 = []\n",
    "t = 0\n",
    "for m in messages:\n",
    "    for i,row in models.iterrows():\n",
    "        m = np.where(m == row['model'], row['brand'], m)\n",
    "        \n",
    "    t+=1\n",
    "    \n",
    "    #print(t)\n",
    "        \n",
    "    messages2.append(m)\n",
    "        \n",
    "df['Message_words_v2'] = messages2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There exists occurences where the text says \"nissan, nissan\"\n",
    "# Remove duplicates in lists while keeping order\n",
    "def remove_consecutive_duplicate (text): #removes consecutive duplicates\n",
    "    return np.array([i for i, j in itertools.groupby(text)])\n",
    "\n",
    "df[\"Message_words_v3\"] = df[\"Message_words_v2\"].apply(remove_consecutive_duplicate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link attributes to models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a unique list of brands in the text\n",
    "brands_list = models.brand.drop_duplicates().to_list()\n",
    "\n",
    "# Add additional brands not covered\n",
    "brands_list.extend(['lexus', 'ferrari', 'merzedesbenz', 'tesla','gm', 'peugeot', 'jeep', 'bentley', 'fiat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = df.Message_words_v3.to_list()\n",
    "\n",
    "# Returns a list of brands mentioned in each review\n",
    "brands_in_message = []\n",
    "for m in messages:\n",
    "    \n",
    "    brands_mentioned = []\n",
    "    for brand in brands_list:\n",
    "        if len(np.where(m == brand)[0]) >0:\n",
    "            brands_mentioned.append(brand)\n",
    "            \n",
    "    brands_in_message.append(brands_mentioned)\n",
    "\n",
    "brands_in_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Brands_in_message'] = brands_in_message\n",
    "df['Qty_brands'] = df.Brands_in_message.apply(lambda x: len(x))\n",
    "df['Qty_brands'].value_counts()\n",
    "\n",
    "# 1291 messages don't mention any brand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assign attributes mentioned in the reviews, like the power of a car, to a specific brand. For example, in the sentence \"I like the BMW for its power. On the other hand, the Honda is reliable\" we would want to assign the the attribute \"power\" to \"BMW\" and \"reliable\" to \"Honda\". We have created functions for two different approaches:\n",
    "1) assign all words found between 1st brand mention and next brand mention to 1st brand\n",
    "\n",
    "2) assign n words to each side of the brand mention to the brand. We found n = 4 to work best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First approach: link all words found between brand mention and next brand mention...\n",
    "# ...except for the first brand mention which receives words from the start of the message\n",
    "\n",
    "m = df.Message_words_v3[9] # record 9 is a good example\n",
    "\n",
    "def get_attr_in_the_right(m, brand_list):\n",
    "\n",
    "    brand_dic = {}\n",
    "    for brand in brands_list:\n",
    "        ix = np.where(m == brand)[0]\n",
    "        if len(ix) > 0:\n",
    "            brand_dic[brand] = ix[0]\n",
    "\n",
    "    # Dict of brand_dic['brand'] = {all words to the right}\n",
    "    brand_dic = dict(sorted(brand_dic.items(), key=lambda x:x[1]))\n",
    "    \n",
    "    # Combined list of all words\n",
    "    brand_list = list(brand_dic.values()) + [len(m)]\n",
    "\n",
    "    review={}\n",
    "    for i, tup in enumerate(brand_dic):        \n",
    "        if i == 0:            \n",
    "            review[tup] = m[0:brand_list[1]]\n",
    "        else:\n",
    "            review[tup] = m[brand_list[i]:brand_list[i+1]]\n",
    "    return review\n",
    "\n",
    "get_attr_in_the_right(m, brands_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this for all records\n",
    "\n",
    "brand_reviews_1st_approach = []\n",
    "\n",
    "for m in df.Message_words_v3:\n",
    "\n",
    "    brand_reviews_1st_approach.append(get_attr_in_the_right(m, brands_list))\n",
    "\n",
    "# generates a list (1 entry per row) of dictionaries {brand_1: part of text corresponding, \n",
    "# brand_2: part of text corresponding,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd approach: consider n words to each side when associating with brands\n",
    "\n",
    "m = df.Message_words_v3[9]\n",
    "\n",
    "def get_attr_n_per_side(m, n, brand_list):\n",
    "\n",
    "    brand_dic = {}\n",
    "    for brand in brands_list:\n",
    "        ix = np.where(m == brand)[0]\n",
    "        if len(ix) > 0:\n",
    "            brand_dic[brand] = ix[0]\n",
    "\n",
    "\n",
    "    brand_dic = dict(sorted(brand_dic.items(), key=lambda x:x[1]))\n",
    "    \n",
    "    #print(len(brand_dic))\n",
    "\n",
    "    brand_list = list(brand_dic.values()) + [len(m)]\n",
    "    \n",
    "    #print(brand_list)\n",
    "\n",
    "    review={}\n",
    "    for i, tup in enumerate(brand_dic):\n",
    "\n",
    "        left_loc = (brand_list[i] - n)\n",
    "        left_loc = max(0, left_loc) # to avoid out of range indexing\n",
    "        \n",
    "        right_loc = (brand_list[i] + n)\n",
    "        right_loc = min(len(m), right_loc) # to avoid OOF indexing\n",
    "        \n",
    "        review[tup] = m[left_loc:right_loc]\n",
    "            \n",
    "    return review\n",
    "\n",
    "get_attr_n_per_side(m, 5, brands_list)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this for all records\n",
    "brand_reviews_2nd_approach = []\n",
    "\n",
    "for m in df.Message_words_v3:\n",
    "\n",
    "    brand_reviews_2nd_approach.append(get_attr_n_per_side(m,4, brands_list))\n",
    "\n",
    "# Generates a list (each row is a post) of dictionaries where {brand_1: part of text corresponding, brand_2: part of text corresponding}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all reviews for each brand into a single key in a dictionary\n",
    "# Creates dictionary: each brand mentioned is a key, value is all words assigned to that brand \n",
    "# According to 1st method get_attr_in_the_right()\n",
    "\n",
    "review_accum_1st = {i:[] for i in brands_list} #deprecated dictionary but still interesting\n",
    "\n",
    "for brand in brands_list:\n",
    "    for review in brand_reviews_1st_approach:\n",
    "        for single_review in review:\n",
    "            #print(review[single_review])\n",
    "            if single_review==brand:\n",
    "                review_accum_1st[brand].extend(review[single_review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above but for 2nd method: get_attr_n_per_side()\n",
    "review_accum_2nd = {i:[] for i in brands_list} \n",
    "\n",
    "for brand in brands_list:\n",
    "    for review in brand_reviews_2nd_approach:\n",
    "        for single_review in review:\n",
    "            #print(review[single_review])\n",
    "            if single_review==brand:\n",
    "                review_accum_2nd[brand].extend(review[single_review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Series with all words \n",
    "words = pd.Series([i for review in df.Message_words_v3 for i in review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get value count for words excluding brands\n",
    "words[~words.isin(brands_list)].value_counts().head(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count = words.value_counts()\n",
    "words_nobrands_count = words[~words.isin(brands_list)].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Stemming could potentially cause information loss if words are unintentionally reduced in a way that would result in a \n",
    "# common stem being used for different words \n",
    "''' Stemming\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_list (text):\n",
    "    return pd.Series([stemmer.stem(w) for w in text])\n",
    "\n",
    "stemmed_words = stem_list(words)\n",
    "len(stemmed_words)\n",
    "\n",
    "stemmed_words[~stemmed_words.isin(brands_list)].value_counts().head(60)\n",
    "\n",
    "stemwords_count = stemmed_words.value_counts()\n",
    "stemwords_nobrands_count = stemmed_words[~stemmed_words.isin(brands_list)].value_counts()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For words_nobrands_count remove words that appear 2x or less\n",
    "words_nobrands_count = words_nobrands_count[words_nobrands_count > 2]\n",
    "words_nobrands_count\n",
    "\n",
    "# This csv is used to create the attribute map\n",
    "#words_nobrands_count.to_csv(\"attribute_count_final.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribute mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute mapping was done by manually browsing rows of xlsx and linking words we found relevant to a common key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map = pd.read_excel(\"attribute_count_final.xlsx\", sheet_name = 0)\n",
    "df_map = df_map.drop('Unnamed: 4', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where no replacement exists, fill it with the original word\n",
    "df_map['attribute_flg'] = df_map['attribute_flg'].replace(np.nan, 0)\n",
    "df_map.attribute_synonym.fillna(df_map.word, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the frequencies of every attribute group after this and show top results \n",
    "df_pivot_map = df_map[df_map.attribute_flg == 1.0].groupby('attribute_synonym').freq.sum().sort_values(ascending=False)\n",
    "df_pivot_map.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 5 attributes are: affordability, sustainability, size, driveability and engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace words with attributes in df.Message_words_v3\n",
    "def replace_attributes(m):\n",
    "    for i,row in df_map.iterrows():\n",
    "        m = np.where(m == row['word'], row['attribute_synonym'], m)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaces attributes (Takes ~20-30 mins to run)\n",
    "df[\"Message_words_v3\"] = df[\"Message_words_v3\"].apply(replace_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Message_words_v2 for size\n",
    "df_saved = df.copy()\n",
    "df = df.drop(\"Message_words_v2\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint csv to avoid long function call later\n",
    "# df.to_pickle(\"data_checkpoint.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating lift scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import checkpoint csv\n",
    "df = pd.read_pickle(\"data_checkpoint.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates lift between a brand and an attribute\n",
    "def calculate_lift(brand_name, attribute_name):\n",
    "    \n",
    "    # Used to get counts in separate table\n",
    "    string_list = []\n",
    "    \n",
    "    # Get total number of reviews\n",
    "    review_count = len(df)\n",
    "    string_list.append(f\"There are {review_count} reviews\")\n",
    "\n",
    "    # Count number of times brand appears across reviews\n",
    "    brand_count = 0\n",
    "    for review in  df.Message_words_v3:\n",
    "        if brand_name in review:\n",
    "            brand_count += 1\n",
    "    string_list.append(f\"{brand_name} appears {brand_count} times\")\n",
    "    \n",
    "    # Get number of times attribute appears across reviews\n",
    "    attribute_count = 0\n",
    "    for review in df.Message_words_v3:\n",
    "            if attribute_name in review:\n",
    "                attribute_count += 1\n",
    "    string_list.append(f\"{attribute_name} appears {attribute_count} times\")\n",
    "\n",
    "    # Count number of times attribute and brand appear together\n",
    "    attr_brand_count = 0\n",
    "    for review in df.Message_words_v3:\n",
    "        if brand_name in review and attribute_name in review:\n",
    "            attr_brand_count += 1\n",
    "    string_list.append(f\"{attribute_name} and {brand_name} appears together {attr_brand_count} times\")\n",
    "    \n",
    "    # Calculate lift\n",
    "    lift = review_count * (attr_brand_count/(brand_count * attribute_count))\n",
    "    \n",
    "    return lift, string_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define top brands and attributes as described in parts A and C/D\n",
    "top_attributes = ['affordability', 'sustainability', 'size', 'driveability', 'engine']\n",
    "top_brands = ['honda','toyota','nissan','volkswagen','chevrolet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataframes to store lift and support values\n",
    "df_lift = pd.DataFrame(index=top_brands, columns = top_attributes)\n",
    "df_lift_counts = pd.DataFrame(index=top_brands, columns = top_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate matrix for lift calculation\n",
    "for brand in top_brands:\n",
    "    for attribute in top_attributes:\n",
    "        df_lift.loc[brand, attribute], df_lift_counts.loc[brand, attribute] = calculate_lift(brand, attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lift_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating lift from brand-attribute assignment lists\n",
    "brand_reviews_1st_approach and brand_reviews_2nd_approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get split reviews with replaced attributes\n",
    "# Generates a list (1 entry per row) of dictionaries {brand_1: part of text corresponding, brand_2: part of text corresponding}\n",
    "brand_reviews_1st_mapped = []\n",
    "\n",
    "for m in df.Message_words_v3:\n",
    "\n",
    "    brand_reviews_1st_mapped.append(get_attr_in_the_right(m, brands_list))\n",
    "\n",
    "brand_reviews_2nd_mapped = []\n",
    "\n",
    "for m in df.Message_words_v3:\n",
    "\n",
    "    brand_reviews_2nd_mapped.append(get_attr_n_per_side(m,4, brands_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate split from list of dictionaries brand_reviews_1st_mapped and brand_reviews_2nd_mapped\n",
    "\n",
    "def calculate_lift_split(brand_name, attribute_name, review_list):\n",
    "    \n",
    "    # Used to get counts in separate table\n",
    "    string_list = []\n",
    "    \n",
    "    # Get total number of reviews - defined as a part of a forum post that talks about a specific brand\n",
    "    # Each review is therefore a split of the entire post, the split being made in 2 different ways as seen above (get_attr_in_the_right, get_attr_n_per_side )\n",
    "    \n",
    "    review_count = 0\n",
    "    for review in review_list: #review_list is a list of dicts, so review is a dict\n",
    "        review_count += len(review)\n",
    "    string_list.append(f\"There are {review_count} reviews about specific brands\")\n",
    "\n",
    "    # Count number of times brand appears across split reviews\n",
    "    brand_count = 0\n",
    "    for review in review_list: #review_list is a list of dicts, so review is a dict\n",
    "        for key in review.keys():\n",
    "            if brand_name == key:\n",
    "                brand_count += 1\n",
    "    string_list.append(f\"{brand_name} appears {brand_count} times\")\n",
    "    \n",
    "    # Count number of times attribute appears across split reviews\n",
    "    attribute_count = 0\n",
    "    for review in review_list:\n",
    "        for value in review.values():\n",
    "            if attribute_name in value:\n",
    "                attribute_count += 1\n",
    "    string_list.append(f\"{attribute_name} appears {attribute_count} times\")\n",
    "\n",
    "    # Count number of times attribute and brand appear together\n",
    "    attr_brand_count = 0\n",
    "    for review in brand_reviews_1st_mapped:\n",
    "        for key, value in review.items():\n",
    "            if brand_name == key and attribute_name in value:\n",
    "                attr_brand_count += 1\n",
    "    string_list.append(f\"{attribute_name} and {brand_name} appears together {attr_brand_count} times\")\n",
    "   \n",
    "    # Calculate lift\n",
    "    lift = review_count * (attr_brand_count/(brand_count * attribute_count))\n",
    "    return lift, string_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataframes\n",
    "df_lift_1st_mapped = pd.DataFrame(index=top_brands, columns = top_attributes)\n",
    "df_lift_1st_counts = pd.DataFrame(index=top_brands, columns = top_attributes)\n",
    "df_lift_2nd_mapped = pd.DataFrame(index=top_brands, columns = top_attributes)\n",
    "df_lift_2nd_counts = pd.DataFrame(index=top_brands, columns = top_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting lift for brand_reviews_1st_mapped\n",
    "for brand in top_brands:\n",
    "    for attribute in top_attributes:\n",
    "        df_lift_1st_mapped.loc[brand, attribute], df_lift_1st_counts.loc[brand, attribute] = calculate_lift_split(brand, attribute, brand_reviews_1st_mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lift_1st_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lift_1st_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting lift for brand_reviews_2nd_mapped\n",
    "for brand in top_brands:\n",
    "    for attribute in top_attributes:\n",
    "        df_lift_2nd_mapped.loc[brand, attribute], df_lift_2nd_counts.loc[brand, attribute] = calculate_lift_split(brand, attribute, brand_reviews_2nd_mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lift_2nd_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_lift_2nd_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
